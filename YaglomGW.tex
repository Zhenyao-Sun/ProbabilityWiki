%%%----Versions-----------------------------
%---YaglomGW14.tex 2018/3/6 by Renming
%---YaglomGW13.tex 2018/3/6 by Yanxia
%---YaglomGW12.tex 2018/3/5 by Zhenyao
%---YaglomGW11.tex 2018/3/3 by Renming
%---YaglomGW10.tex 2018/3/3 by Zhenyao---
%---YaglomGW.tex 2017/6/21 submited to Arxiv.org by Zhenyao---
%---YaglomGW.pdf 2017/4/21 submitted to JAP by Zhenyao---
%---YaglomGW9.tex 2017/4/21 by Zhenyao-----------------
%---YaglomGW8.tex 2017/4/21 by Renming-----------------
%---YaglomGW7.tex 2017/4/21 by Yanxia-----------------
%---YaglomGW6.tex 2017/4/19 by Renming-----------------
%---YaglomGW5.tex 2017/4/18 by Yanxia---------------
%---YaglomGW4.tex 2017/4/16 by Zhenyao---------------
%---YaglomGW3.tex 2017/4/11 by Renming-----------------
%---YaglomGW2.tex 2017/4/11 by Yanxia-----------------
%---YaglomGW.tex 2017/4/6 by Zhenyao-----------------
%%%----Documentstyles-----------------------
\documentclass[12pt,a4paper]{amsart}
\setlength{\textwidth}{\paperwidth}\addtolength{\textwidth}{-2in}\calclayout
\usepackage{hyperref}
%\usepackage{nicefrac}
\usepackage{comment}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%%%----Environments------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\numberwithin{equation}{section}

%%%---------Top matter----------------
\title[A 2-spine decomposition and Yaglom's theorem]
{\large A 2-spine Decomposition of the Critical Galton-Watson Tree and a Probabilistic Proof of Yaglom's Theorem}
\author{Yan-Xia Ren, Renming Song and Zhenyao Sun}
%-------Yan-Xia Ren--------------
\address{
	Yan-Xia Ren\\
	School of Mathematical Sciences \& Center for Statistical Science\\
	Peking University, Beijing\\
	P. R. China, 100871}
\email{yxren@math.pku.edu.cn}
\thanks{The research of Yan-Xia Ren is supported in part by NSFC (Grant Nos. 11671017 and 11731009)}
%--------Renming Song------------
\address{
	Renming Song\\
	Dept of Mathematics\\
	University of Illinois at Urbana-Champaign\\
	Urbana, IL 61801}
\email{rsong@illinois.edu}
\thanks{The research of Renming Song is supported in part by the Simons Foundation (\#429343, Renming Song)}
%--------Zhenyao Sun------------
\address{
	Zhenyao Sun\\
	School of Mathematical Sciences\\
	Peking University\\
	Beijing, P. R. China, 100871}
\email{zhenyao.sun@pku.edu.cn}
\thanks{Zhenyao Sun is supported by the China Scholarship Council. Corresponding author.}
%------Footnotes----------------------
\keywords{
	Galton-Watson process, Galton-Watson tree, spine decomposition, Yaglom's theorem, martingale change of measure}
\subjclass[2010]{60J80, 60F05}
%%%----Main-------------------------------
\begin{document}
\begin{abstract}
	In this note  we propose a two-spine decomposition of the critical Galton-Watson tree and use this decomposition to give a probabilistic proof of Yaglom's theorem.
\end{abstract}
\maketitle	
\section{Introduction}
\subsection{Model}
\label{sec:model}
	Consider a critical Galton-Watson process $\{(Z_n)_{n\ge 0}; \mathbf P \}$ with $Z_0 = 1$ and offspring distribution $\mu$ on $\mathbb N_0 : = \{0,1,\dots\}$ which has mean $1$ and finite non-degenerate variance $\sigma^2$, i.e.,
\begin{equation}\label{eq:mean}
 \sum_{k=0}^\infty k \mu(k)	=1
\end{equation}
	and
\begin{equation}\label{eq:variance}
	0	
	<	\sigma^2
	:=	\sum_{k=0}^\infty  (k-1)^2 \mu(k)
	=	\sum_{k=0}^\infty k(k-1) \mu(k)
	<	\infty.
\end{equation}
	For simplicity, we will refer to $\{(Z_n); \mathbf P\}$ as  a  \emph{$\mu$-Galton-Watson process}.
	It is well known that
\begin{thm}[\cite{kesten1966galton}] \label{thm: Kolmogrov and Yaglom theorem}
	For a $\mu$-Galton-Watson process $\{(Z_n); \mathbf P\}$ satisfying \eqref{eq:mean} and \eqref{eq:variance}, we have
\begin{enumerate}
\item \label{thm:kolmogorov}
	$n \mathbf P \{Z_n>0\} \xrightarrow[n \to \infty]{} \frac{2}{\sigma^2};$
\item \label{thm:yaglom}
	$\{n^{-1}Z_n; \mathbf P(\cdot | Z_n>0)\}\xrightarrow[n \to \infty]{d} Y,$
\end{enumerate}
	where $Y$ is an exponential random variable with mean $\sigma^2/2$.
\end{thm}

	Under a third moment assumption, assertions \eqref{thm:kolmogorov} and \eqref{thm:yaglom} of Theorem \ref{thm: Kolmogrov and Yaglom theorem} are due to \cite{kolmogorov1938losung} and \cite{yaglom1947certain} respectively.
	Theorem \ref{thm: Kolmogrov and Yaglom theorem}(2) is usually called Yaglom's theorem.
	For probabilistic proofs of the above results, we refer our readers to
\cite{geiger1999elementary}, \cite{geiger2000new} and \cite{lyons1995conceptual}.

	In \cite{lyons1995conceptual}, Lyons, Pemantle and Peres gave a probabilistic proof of Theorem \ref{thm: Kolmogrov and Yaglom theorem} using the so-called size-biased $\mu$-Galton-Watson tree.
	In this note, by \emph{size-biased transform} we mean the following:
	Let $\{X;P\}$ be a random element and $g(X)$ be a Borel function of $X$ with $P(g(X) \geq 0) = 1$ and $P[g(X)]\in (0,\infty)$.
	We say a random element $\{\dot X;\dot P\}$ is 
%	the $g(X)$-size-biased transform (or simply $g(X)$-transform) of $X$ if
   a $g(X)$-size-biased transform (or simply $g(X)$-transform) of $X$ if
	\[
		\dot P[f(\dot X)] = \frac{ P[g(X)f(X)]}{P[g(X)]}
	\]
	for each positive Borel function $f$.

	We now recall the size-biased $\mu$-Galton-Watson tree introduced in \cite{lyons1995conceptual}.
	Let $L$ be a random variable with distribution $\mu$.
%	Denote by $\dot L$ the \emph{$L$-transform} of $L$.
   Denote by $\dot L$ an \emph{$L$-transform} of $L$.
	The celebrated \emph{size-biased $\mu$-Galton-Watson tree} is then constructed as follows:
\begin{itemize}
\item
	There is an initial particle which is marked.
\item
	Any marked particle gives independent birth to a random number of children according to $\dot L$. Pick one of those children randomly as the new marked particle while leaving the other children as unmarked particles.
\item
	Any unmarked particle gives independent birth to a random number of unmarked children according to $L$.
\item
	The evolution goes on.
\end{itemize}

	Notice that the marked particles form a descending family line which will be referred to as the \emph{spine}.
	Define $\dot Z_n$ as the population of the $n$th generation in the size-biased tree.
	It is proved in \cite{lyons1995conceptual} that the process $(\dot Z_n)_{n\ge 0}$ is a martingale transform of the process $(Z_n)_{n\ge 0}$ via the martingale $(Z_n)_{n\ge 0}.$
	That is, for any generation number $n$ and any bounded Borel function $g$ on $\mathbb N_0^{n}$,
\begin{equation}
\label{eq:htransformation}
	\mathbf E [ g ( \dot Z_1, \dots, \dot Z_n) ]
	= \frac { \mathbf E[ Z_n g( Z_1, \dots, Z_n)]} {\mathbf E [ Z_n]}.
\end{equation}

	It is natural to consider probabilistic proofs of analogous results of Theorem \ref{thm: Kolmogrov and Yaglom theorem} for more general critical branching processes.
	Vatutin and  Dyakonova \cite{VD} gave a probabilistic proof of Theorem \ref{thm: Kolmogrov and Yaglom theorem}(1) for multitype critical branching processes.
	As far as we know, there is no probabilistic proof of Yaglom's theorem for multitype critical branching processes.
	It seems that it is difficult to adapt the probabilistic proofs in \cite{geiger2000new} and \cite{lyons1995conceptual} for monotype branching processes to more general models, such as multitype branching processes, branching Hunt processes and superprocesses.

	In this note, we propose a $k(k-1)$-type size-biased $\mu$-Galton-Watson tree equipped with a two-spine skeleton, which serves as a change-of-measure of the original $\mu$-Galton-Watson tree;
	and with the help of this two-spine technique, we give a new probabilistic proof of Theorem \ref{thm: Kolmogrov and Yaglom theorem}(2), i.e. Yaglom's theorem.
	The main motivation for developing this new proof for the classical Yaglom's theorem is that this new method is generic, in the sense that it can be generalized to more complicated critical branching systems.
	In fact, in our followed-up paper \cite{RenSongSun2017Spine}, we show that, in a similar spirit, a two-spine structure can be constructed for a class of critical superprocesses, and a probabilistic proof of a Yaglom type theorem can be obtained for those processes.



	Another aspect of our new proof is that we take advantage of a fact that the exponential distribution can be characterized by a particular $x^2$-type size-biased distributional equation.
	An intuitive explanation of our method on the exponential convergence, and a comparison with the methods of \cite{geiger2000new} and \cite{lyons1995conceptual}, are
	made in the next subsection.
	We think this new point of view of convergence to the exponential law provides an alternative insight on the classical Yaglom's theorem.

	We now give a formal construction of our $k(k-1)$-type size-biased $\mu$-Galton-Watson tree.
%	Denote by $\ddot L$ the \emph{$L(L-1)$-transform} of $L$.
	Denote by $\ddot L$ an \emph{$L(L-1)$-transform} of $L$.
	Fix a generation number $n$ and pick a random generation number $K_n$ uniformly among $\{0,\dots,n-1\}$.
	The \emph{$k(k-1)$-type size-biased $\mu$-Galton-Watson tree with height $n$} is then defined as a particle system such that:
\begin{itemize}
\item
	There is an initial particle which is marked.
\item
	Before or after generation $K_n$, any marked particle gives independent birth to a random number of children according to $\dot L$.
	Pick one of those children randomly as the new marked particle while leaving the other children as unmarked particles.
\item
	The marked particle at generation $K_n$, however, gives independent birth to a random number of children according to $\ddot L$.
	Pick two different particles randomly among those children as the new marked particles while leaving the other children as unmarked particles.
\item
	Any unmarked particle gives independent birth to a random number of unmarked children according to $L$.
\item
	The system stops at generation $n$.
\end{itemize}

	If we track all the marked particles, it is clear that they form a \emph{two-spine skeleton} with $K_n$ being the last generation where those two spines are together.
	It would be helpful to consider this skeleton as two disjoint spines,
	where \emph{the longer spine} is a family line from generation $0$ to $n$ and \emph{the shorter spine} is a family line from generation $K_n+1$ to $n$.
	
	For any $0\le m \le n$, denote by $\ddot Z_m^{(n)}$ the population of the $m$th generation in the $k(k-1)$-type size-biased $\mu$-Galton-Watson tree with height $n$.
	The main reason for proposing such a model is that the process $(\ddot Z_m^{(n)})_{0\le m\le n}$ can be viewed as 
%	the $Z_n(Z_n-1)$-transform of the process $(Z_m)_{0\le m\le n}$.
    a $Z_n(Z_n-1)$-transform of the process $(Z_m)_{0\le m\le n}$.
	This is made precise in the result below which will be proved in Section \ref{sec:spacesandmeasures}.
\begin{thm}
\label{thm: change of measure}
	Let $(Z_m)_{m\ge 0}$ be a $\mu$-Galton-Watson process and $(\ddot Z_m^{(n)})_{0\le m\le n}$ be the population of a $k(k-1)$-type size-biased $\mu$-Galton-Watson tree with height $n$.
	Suppose that $\mu$ satisfies \eqref{eq:mean} and \eqref{eq:variance}.
	Then, for any bounded Borel function $g$ on $\mathbb N^{n}_0$,
\begin{equation*}
		\mathbf P[ g ( \ddot Z_1^{(n)}, \dots, \ddot Z_n^{(n)})]
	=
		\frac{ \mathbf P[ Z_n(Z_n-1) g( Z_1, \dots, Z_n)]} {\mathbf P [ Z_n ( Z_n - 1)]}.			
\end{equation*}
\end{thm}

	The idea of considering a branching particle system with more than one spine is not new.
	A particle system with $k$ spines  was constructed in \cite{harris2015many} and used in the  many-to-few formula for branching Markov processes and branching random walks.
	Inspired by \cite{harris2015many}, we use a two-spine model to characterize the $k(k-1)$-type size-biased branching process.


\subsection{Methods.}
	As mentioned above, our method of proving Yaglom's theorem takes advantage of a fact that the exponential distribution is characterized by an $x^2$-type size-biased distributional equation.
	This is made precise in the next lemma, which will be proved in Section \ref{sec: proofs}:
\begin{lem} \label{lem: our equation}
	Let $Y$ be a strictly positive random variable with finite second moment.
	Then $Y$ is exponentially distributed if and only if
\begin{equation}
\label{eq: x2 type size-biased equation for exponential distribution}
	\ddot Y \overset{d}
	= \dot Y + U \cdot \dot Y'.
\end{equation}
%	Here $\dot Y$ and $\dot Y'$ are both $Y$-transform of  $Y$, 
	Here $\dot Y$ and $\dot Y'$ are both $Y$-transforms of  $Y$, 
%	$\ddot Y$ is the $Y^2$-transform of $Y$, 
$\ddot Y$ is a $Y^2$-transform of $Y$, 
	$U$ is a uniform random variable on $[0,1]$, and $\dot Y$, $\dot Y'$, $\ddot Y$ 
	and $U$ are independent.
\end{lem}	
	With this lemma and Theorem \ref{thm: change of measure}, we can give an intuitive explanation of the exponential convergence in Yaglom's Theorem.
	From the construction of the $k(k-1)$-type size-biased $\mu$-Galton-Watson tree $(\ddot Z^{(n)}_m)$, we see that the population $\ddot Z^{(n)}_n$ in 
%	$n$th generation can be separated into two parts: descendants 
   the $n$th generation can be separated into two parts: descendants 
	from the longer spine and descendants from the shorter spine.
	Due to their construction,
	the first part, the descendants from the longer spine at generation $n$,
	is distributed approximately like $\dot Z_n$,
	while the second part, the descendants from the shorter spine at generation $n$,
		is distributed approximately like $\dot Z_{ \floor{U\cdot n}}$.
	Those two parts are approximately independent of each other.
	So, after a renormalization, we have roughly that
\begin{equation}
\label{eq: Our insight}
	\frac{\ddot Z_n^{(n)}}{n}
	\overset{d} \approx \frac{\dot Z_n}{n} + 
			U \cdot \frac{   \dot Z'_{  \floor{ U \cdot n }  }   }    {   Un   },
\end{equation}
%		where process $(\dot Z'_m)$ is an independent copy of $(\dot Z_m)$.
		where the process $(\dot Z'_m)$ is an independent copy of $(\dot Z_m)$.
	Therefore, if $Y$ is the weak limit of $Z_n/n$ conditioned on $\{Z_n > 0\}$,
	then $Y$ should satisfy \eqref{eq: x2 type size-biased equation for exponential distribution}, which suggests that $Y$ is exponentially distributed.
	%RS I do not really understand this sentence.
	
	It would be interesting to compare this view of exponential convergence with the methods used in \cite{geiger2000new} and \cite{lyons1995conceptual}.
	In \cite{lyons1995conceptual}, Lyons, Pemantle and Peres characterize the exponential distribution by a different 
%	but commonly known $x$-type size-biased distributional equation:
but well-known $x$-type size-biased distributional equation:
	A nonnegative random variable $Y$ with positive finite mean is exponentially distributed if and only if it satisfies that
\begin{equation}
\label{eq: Lyons' distributional equation}
		Y 		\overset{d}= U \cdot \dot Y
\end{equation}
%    where $\dot Y$ is the $Y$-transform of $Y$,  and $U$ is  uniform random variable on 
   where $\dot Y$ is a $Y$-transform of $Y$,  and $U$ is a uniform random variable on 
$[0,1]$, which is independent of $\dot Y$.
    With the help of the size-biased tree, they then show that $\ceil{U \cdot \dot Z_n}$ is distributed approximately like $Z_n$ conditioned on $\{Z_n > 0\}$.
	So, after a renormalization, they have roughly that
\begin{equation}
\label{eq: Lyons' insight}
	\big\{\frac{Z_n}{n} ; \mathbf P(  \cdot| Z_n > 0) \big\}
	\overset{d}{\approx} U \cdot \frac{ \dot Z_n}{n}
\end{equation}
	and therefore, if $Y$ is the weak limit of $Z_n/n$ conditioned on $\{Z_n > 0\}$,
	then $Y$ should satisfy \eqref{eq: Lyons' distributional equation}, which suggests that $Y$ is exponentially distributed.
	
	In \cite{geiger2000new}, Geiger characterizes the exponential distribution by another distributional equation:
		If $Y^{(1)}$ and $Y^{(2)}$ are independent copies of an exponential random variable $Y$, and $U$ is again an independent uniform random variable on $[0,1]$, then
\begin{equation}
\label{eq: Geiger's equation}
	Y	\overset{d} = U (Y^{(1)} + Y^{(2)}).
\end{equation}
%	Geiger then shows for $(Z_n)$ that, conditioned on non-extinction at $n$, 
	Geiger then shows that for $(Z_n)$, conditioned on non-extinction at $n$, 
	the distribution of the generation of the most recent common ancestor (MRCA) of the particles at generation $n$ is asymptotically uniform among $\{0,1,\dots,n\}$ (a result due to \cite{Zubkov1975}, see also \cite{geiger1999elementary},) and there are asymptotically two children of 
%	the MRCA with a descendant at $n$.
the MRCA, each with at least 1 descendant in generation $n$.
	After a renormalization, roughly speaking, Geiger has that
\begin{equation}
\label{eq: Geiger's insight}
	\big\{\frac{Z_n}{n} ; \mathbf P(  \cdot| Z_n > 0) \big\}
		\overset{d}{\approx} U \cdot \frac{ Z^{(1)}_{  \floor{Un}  }}{Un} + U \cdot \frac{ Z^{(2)}_{ \floor{Un} }}{Un} ,
\end{equation}
	where for each $m$, $Z_m^{(1)}$ and $Z_m^{(2)}$ are independent copies of $\{Z_m; \mathbf P(\cdot | Z_m > 0)\}$.
	And therefore, if $Y$ is the weak limit of $Z_n/n$ conditioned on $\{Z_n > 0\}$, then $Y$ should satisfy \eqref{eq: Geiger's equation}, which suggests that $Y$ is exponentially distributed.
	
	From this comparison, we see that all the methods mentioned above shares one similarity: They all establish the exponential convergence via some particular distributional equation.
	However, since the equations \eqref{eq: x2 type size-biased equation for exponential distribution}, \eqref{eq: Lyons' distributional equation} and \eqref{eq: Geiger's equation} are different, the actual way of proving the convergence varies.
	In \cite{lyons1995conceptual}, an elegant tightness argument is made along with \eqref{eq: Lyons' insight}.
	However, it seems that this tightness argument is not suitable for \eqref{eq: Geiger's insight}, due to a property that the conditional convergence for some subsequence $Z_{n_k}/n_k$ implies the convergence of $U \cdot \dot Z_{n_k}/n_k$, 
%   	but dose not implies the convergence of $Z^{(i)}_{ \floor{Un_k}}/Un_k, i=1, 2$.
   	but does not implies the convergence of $Z^{(i)}_{ \floor{Un_k}}/Un_k, i=1, 2$.
	Instead, a contraction type argument in the $L^2$-Wasserstein metric is used by Geiger \cite{geiger2000new}.
	
	Due to similar reasons, in this note,
	to actually prove the exponential convergence using \eqref{eq: Our insight} and \eqref{eq: x2 type size-biased equation for exponential distribution}, some efforts also must be made.
	We observe that the distributional equation \eqref{eq: Our insight} admits 
%	the so-called size-biased add-on structure, which is related to L\`evy's 
    a so-called size-biased add-on structure, which is related to L\`evy's 
	theory of infinitely divisible distributions: Suppose that $X$ is a nonnegative random variable with $ a := E [X]\in (0,\infty)$,
	then $X$ is infinitely divisible if and only if there exists a nonnegative random variable $A$ independent of $X$ such that $\dot X 	\overset{d} = X + A$.
	In fact, see \cite[Theorem 10.1]{ArratiaGoldsteinKochman2013}, the Laplace exponent of $X$ can be expressed as
\[
	-\ln E[ e^{-\lambda X}]
	 =  a \alpha(\{0\}) \lambda+ a \int_{(0,\infty)} \frac{1 - e^{-\lambda y}}{y} \alpha(dy)
\]
	where $\alpha$ is the distribution of $A$.
	Moreover, if $A$ is strictly positive, then
\begin{equation}\label{eq: Laplace exponent for size-biased add-on equation}
	-\ln E[ e^{-\lambda X}]
	=  a  \int_0^\lambda E [e^{-s A}] ds.
\end{equation}
	From this point of view, after considering the Laplace transform of \eqref{eq: Our insight} and \eqref{eq: x2 type size-biased equation for exponential distribution}, we can establish the convergence of $E[e^{-\lambda \dot Z_n/n}]$ to $E[e^{-\lambda \dot {Y}}]$, which will eventually lead us to Yaglom's theorem.
	This is made precise in Section \ref{sec: proofs}.
	A similar type of argument is also  used in our followed-up paper \cite{RenSongSun2017Spine}%for the critical superprocesses.
	for critical superprocesses.
	
	
\section{Trees and their decompositions}
\label{sec:preliminary}
\subsection{Spaces and measures}
\label{sec:spacesandmeasures}
	In this subsection, we give a proof of Theorem \ref{thm: change of measure}.
 	Consider \emph{particles} as elements in the space
\begin{equation*}
		\mathcal U
	:=
	\{\emptyset\}\cup\bigcup_{k=1}^\infty \mathbb N^k,
\end{equation*}
	where $\mathbb N:=\{1,2,\dots\}$.
	Therefore elements in $\mathcal U$ are of the form 213, which we read as the individual being the 3rd child of the 1st child of the 2nd child of the initial ancestor $\emptyset$.
	For two particles $u=u_1\dots u_n, v=v_1\dots v_m\in\mathcal U$, $uv$ denotes the concatenated particle $uv:=u_1\dots u_nv_1\dots v_m$.
	We use the convention $u\emptyset = \emptyset u = u$ and $u_1\dots u_n=\emptyset$ if $n=0$.
	For any particle $u:=u_1\dots u_{n-1}u_n$, we define its \emph{generation} as $| u |:=n$ and its \emph{parent particle} as $\overleftarrow{u}:=u_1\dots u_{n-1}$.
	For any particle $u \in \mathcal U$ and any subset $\mathbf a \subset \mathcal U$, we define the \emph{number of children of $u$ in $\mathbf a$} as $l_u(\mathbf a) := \#\{\alpha\in \mathbf a:\overleftarrow{\alpha}=u\} $.
	We also define the \emph{height} of $\mathbf a$ as $|\mathbf a|:=\sup_{\alpha\in \mathbf a}|\alpha|$ and its \emph{population in the $n$th generation} as $X_n(\mathbf a):=\#\{u\in \mathbf a:|u|=n\}$.
	A \emph{tree} $ \mathbf t $ is defined as a subset of $\mathcal U$ such that there exists an $\mathbb N_0$-valued sequence $(l_u)_{u\in \mathcal U}$,
	indexed by $\mathcal U$, satisfying
\begin{equation*}
		 \mathbf t
	=\{u_1\dots u_m\in \mathcal U: m\ge 0, u_j\leq l_{u_1\dots u_{j-1}}, \forall  j=1,\dots,m\}.
\end{equation*}
	A \emph{spine} $ \mathbf v$ on a  tree $ \mathbf t $ is defined as a sequence of particles $\{v^{(k)}:k=0,1,\dots,| \mathbf t |\}\subset \mathbf t $ such that $v^{(0)}=\emptyset$ and $\overleftarrow{v^{(k)}}=v^{(k-1)}$ for any $k=1,\dots, | \mathbf t |$.
	In the case that $| \mathbf t |=\infty$, we simply write $k=0,1,\dots$ as $k=0,1,\dots, | \mathbf t |$.

	Fix a generation number $n\in \mathbb N$. Define the following spaces:
\begin{itemize}
\item
	\emph{The space of trees with height no more than $n$},
\begin{equation*}
		\mathbb T_{\leq n}
	:=\{ \mathbf t : \mathbf t \text{ is a tree with }| \mathbf t | \leq n\}.
\end{equation*}
\item
	\emph {The space of $n$-height trees with one distinguishable spine},
\begin{equation*}
		\dot{\mathbb T}_n
	:=\{( \mathbf t , \mathbf v): \mathbf t  \text{ is a tree with } | \mathbf t |=n,  \mathbf v \text{ is a spine on }  \mathbf t \}.
\end{equation*}
\item
	\emph{The space of $n$-height trees with two different distinguishable spines},
\begin{equation*}
		\ddot{\mathbb T}_n
	:=\{( \mathbf t , \mathbf v, \mathbf v'):( \mathbf t , \mathbf v)\in\dot{\mathbb T}_n,( \mathbf t , \mathbf v')\in\dot{\mathbb T}_n, \mathbf v\neq \mathbf v'\}.
\end{equation*}
\end{itemize}

	Let $(L_u)_{u\in\mathcal U}$ be a collection of independent random variables with law $\mu$, indexed by $\mathcal U$.
	Denote by $T$ the random tree defined by
\begin{equation*}
		T
	:=\{u_1\dots u_m\in \mathcal U: 0\le m\le n, u_j\leq L_{u_1\dots u_{j-1}},\forall j=1,\dots,m\}.
\end{equation*}
	We refer to $T$ as a \emph{$\mu$-Galton-Watson tree with height no more than n} since its population $(X_m(T))_{0\le m\le n}$ is a $\mu$-Galton-Watson process stopped at generation $n$.
	Define the \emph{$\mu$-Galton-Watson measure $\mathbf G_n$} on $\mathbb T_{\leq n}$ as the law of the random tree $T$.
	That is, for any $ \mathbf t \in\mathbb T_{\leq n}$,
\begin{equation*}
		\mathbf G_n( \mathbf t )
    :=\mathbf P(T= \mathbf t )
	=\mathbf P(L_u=l_u( \mathbf t )\text{ for any } u\in \mathbf t  \text{ with }|u|<n)
	=\prod_{u\in  \mathbf t :|u|<n}\mu(l_u( \mathbf t )).
\end{equation*}

%	Recall that $\dot L$ is the $L$-transform of $L$.
	Recall that $\dot L$ is an $L$-transform of $L$.
	Define $\dot C$ as a random number which, conditioned on $\dot L$, is uniformly distributed on $\{1,\dots,\dot L\}$.
	Independent of $(L_u)_{u\in\mathcal U}$, let $(\dot L_u,\dot C_u)_{u\in \mathcal U}$ be a collection of independent copies of $(\dot L,\dot C)$, indexed by $\mathcal U$.
	We then use $(L_u)_{u\in\mathcal U}$ and $(\dot L_u,\dot C_u)_{u\in\mathcal U}$ as the building blocks to construct the size-biased $\mu$-Galton-Watson tree $\dot T$ and its distinguishable spine $\dot V$ following the steps described in Section \ref{sec:model}.
	We use $L_u$ as the number of children of particle $u$ if $u$ is unmarked and use $\dot L_u$ if $u$ is marked.
	In the latter case, we always set the $C_u$-th child of $u$, i.e. particle $uC_u$, as the new marked particle.
	For convenience, we stop the system at generation $n$. To be precise, the random spine $\dot V$ is defined by
\begin{equation*}
		\dot V
	:=\{v_1\dots v_m\in \mathcal U:0\le m\le n, v_j=\dot C_{v_1\dots v_{j-1}},\forall j=1,\dots,m\},
\end{equation*}
	and the random tree $\dot T$ is defined by
\begin{equation*}
		\dot T
	:=\{u_1\dots u_m\in\mathcal U: 0\le m\le n,u_j\leq \tilde L_{u_1\dots u_{j-1}},\forall j=1,\dots,m\},
\end{equation*}
	where, for any $u\in\mathcal U$, $\tilde L_u:=L_u\mathbf 1_{u\not\in \dot V}+\dot L_u\mathbf 1_{u\in \dot V}$.

	We now consider the distribution of the $\dot{\mathbb T}_n$-valued random element $(\dot T,\dot V)$.
	For any $( \mathbf t , \mathbf v)\in\dot{\mathbb T}_n$, the event $\{(\dot T,\dot V)=( \mathbf t , \mathbf v)\}$ occurs if and only if:
\begin{itemize}
\item
    $L_u=l_u( \mathbf t )$ for each $u\in  \mathbf t \setminus \mathbf v$ with $| u |<n$ and
\item
	$(\dot L_{v_1\dots v_m},\dot C_{v_1\dots v_m})=(l_{v_1\dots v_m}( \mathbf t ),v_{m+1})$ for each $v_1\dots v_{m+1}\in \mathbf v$ with $0\le m\le n-1$.
\end{itemize}
    Therefore, the distribution of $(\dot T,\dot V)$ can be determined by
\begin{equation}
\label{eq:treespinemeasure}
		\mathbf P((\dot T,\dot V)=( \mathbf t , \mathbf v))
	=\prod_{u\in  \mathbf t \setminus \mathbf v:|u|<n}\mu(l_u( \mathbf t ))
	\cdot \prod_{u\in  \mathbf v:| u| <n}l_u( \mathbf t )\mu(l_u( \mathbf t ))\frac{1}{l_u( \mathbf t )}
	= \mathbf G_n( \mathbf t ).
\end{equation}
	
	The \emph{size-biased $\mu$-Galton-Watson measure $\dot {\mathbf G}_n$} on $\mathbb T_{\leq n}$ is then defined as the law of the $\mathbb T_{\leq n}$-valued random element $\dot T$. That is, for any $ \mathbf t \in\mathbb T_{\leq n}$,
\begin{equation}
\label{eq:sizebiasedGWmeasure}
\begin{split}
		\dot {\mathbf G}_n( \mathbf t )
	&:= \mathbf P(\dot T= \mathbf t )
	= \sum_{ \mathbf v:( \mathbf t , \mathbf v)\in \dot{\mathbb T}_n} \mathbf P((\dot T,\dot V)=( \mathbf t , \mathbf v))
	\\&= \#\{ \mathbf v:( \mathbf t , \mathbf v)\in \dot{\mathbb T}_n\} \cdot \mathbf G_n( \mathbf t )
	= X_n( \mathbf t ) \cdot \mathbf G_n( \mathbf t ).
\end{split}
\end{equation}

	Equations \eqref{eq:treespinemeasure}, \eqref{eq:sizebiasedGWmeasure} and their consequence \eqref{eq:htransformation} were first obtained in \cite{lyons1995conceptual}.
	We use these equations to help us to understand how the $k(k-1)$-type size-biased $\mu$-Galton-Watson tree can be represented.
	
	Recall that $K_n$ is a random generation number uniformly distributed on $\{0,\dots,n-1\}$, 
%	and $\ddot L$ is the $L(L-1)$-transform of $L$.
	and $\ddot L$ is an $L(L-1)$-transform of $L$.
	Define $(\ddot C,\ddot C')$ as a random vector which, conditioned on $\ddot L$, is uniformly distributed on $\{(i,j)\in\mathbb N^2:1\leq i\neq j\leq \ddot L\}$.
	Suppose that $(L_u)_{u\in\mathcal U}, (\dot L_u,\dot C_u)_{u\in \mathcal U}$, $(\ddot L,\ddot C,\ddot C')$ and $K_n$ are independent of each other.
	We now use these elements to build the $k(k-1)$-type size-biased $\mu$-Galton-Watson tree $\ddot T$ and its two different distinguishable spines $\ddot V$ and $\ddot V'$ following the steps described in Section \ref{sec:model}.
	Write $C_u:=\dot C_u\mathbf 1_{|u|\neq K_n}+\ddot C\mathbf 1_{|u|=K_n}$ and $C'_u:=\dot C_u\mathbf 1_{|u|\neq K_n}+\ddot C'\mathbf 1_{|u|=K_n}$.
	We define the random spines $\ddot V$ and $\ddot V'$ as
\begin{align*}
        \ddot V
	&:= \{v_1\dots v_m\in \mathcal U:0\le m\le n, v_j= C_{v_1\dots v_{j-1}},\forall j=1,\dots,m\},
	\\ \ddot V'
	&:= \{v_1\dots v_m\in \mathcal U:0\le m \le n, v_j= C'_{v_1\dots v_{j-1}},\forall j=1,\dots,m\},
\end{align*}
	and the random tree $\ddot T$ as
\begin{equation*}
	    \ddot T
	:=
		\{u_1\dots u_m\in\mathcal U: 0\le m\le n,u_j\leq L''_{u_1\dots u_{j-1}},\forall j=1,\dots,m\},
\end{equation*}
	where, for any $u\in\mathcal U$, $L''_u:=L_u \mathbf 1_{u\not\in \ddot V\cup\ddot V'}+\dot L_u \mathbf 1_{u\in \ddot V\cup\ddot V',|u|\neq K_n}+\ddot L\mathbf 1_{u\in \ddot V\cup\ddot V',|u|=K_n}$.

	We now consider the distribution of $(\ddot T,\ddot V,\ddot V')$.
	For any $( \mathbf t , \mathbf v, \mathbf v')\in\ddot {\mathbb T}_n$, the event $\{(\ddot T,\ddot V,\ddot V')=( \mathbf t , \mathbf v, \mathbf v')\}$ occurs if and only if:
\begin{itemize}
\item
    $K_n=k_n:=| \mathbf v\cap \mathbf v'|$,
\item
    $L_u=l_u( \mathbf t )$ for each $u\in  \mathbf t \setminus( \mathbf v\cup \mathbf v')$ with $| u| <n$,
\item
	$(\dot L_{v_1\dots v_m},\dot C_{v_1\dots v_m})=(l_{v_1\dots v_m}( \mathbf t ),v_{m+1})$ for each $v_1\dots v_mv_{m+1}\in \mathbf v\cup \mathbf v'$ with $k_n\neq m<n$ and
\item
	$(\ddot L,\ddot C,\ddot C')=(l_{v_1\dots v_{k_n}}( \mathbf t ),v_{k_n+1},v'_{k_n+1})$ for $v_1\dots v_{k_n}v_{k_n+1}\in \mathbf v$ and $v_1\dots v_{k_n}v'_{k_n+1}\in \mathbf v'$.
\end{itemize}
	Using this analysis, one can verify that
\begin{align*}
		&\mathbf P\big((\ddot T,\ddot V,\ddot V')=( \mathbf t , \mathbf v, \mathbf v')\big)\\
	&\quad= \frac{1}{n} \cdot \prod_{u\in  \mathbf t \setminus( \mathbf v\cup  \mathbf v'):|u|<n} \mu(l_u( \mathbf t )) \cdot \prod_{u\in  \mathbf v\cup  \mathbf v':k_n\neq|u|<n}l_u( \mathbf t ) \mu(l_u( \mathbf t ))\frac{1}{l_u( \mathbf t )}
    \\&\qquad \cdot \prod_{u\in  \mathbf v \cup  \mathbf v':|u|=k_n}\frac{l_u( \mathbf t )(l_u( \mathbf t )-1) \mu(l_u( \mathbf t ))}{\sigma^2}\frac{1}{l_u( \mathbf t )(l_u( \mathbf t )-1)}\\
	&\quad = \frac{1}{n\sigma^2} \mathbf G_n( \mathbf t ).
\end{align*}
	
	The \emph{$k(k-1)$-type size-biased $\mu$-Galton-Watson measure $\ddot{\mathbf G}_n$} on $\mathbb T_{\leq n}$ is then defined as the law of the random element $\ddot T$. That is, for any $ \mathbf t \in\mathbb T_{\leq n}$,
\begin{equation}
\label{eq:k(k-1)typesizebiasedGWmeasure}
\begin{split}
		\ddot{\mathbf G}_n( \mathbf t )
	&:= \mathbf P(\ddot T= \mathbf t )
	= \sum_{( \mathbf v, \mathbf v'):( \mathbf t , \mathbf v, \mathbf v')\in \ddot {\mathbb T}_n} \mathbf P\big((\ddot T,\ddot V,\ddot V')=( \mathbf t , \mathbf v, \mathbf v')\big)
	\\&= \#\{( \mathbf v, \mathbf v'):( \mathbf t , \mathbf v, \mathbf v')\in \ddot {\mathbb T}_n\} \cdot \frac{\mathbf G_n( \mathbf t )}{n\sigma^2}
	= \frac{X_n( \mathbf t )(X_n( \mathbf t )-1)}{n\sigma^2} \cdot{\mathbf G}_n( \mathbf t ).
\end{split}
\end{equation}

	We note in passing that, because of the way they are constructed, the measures $(\ddot{\mathbf G}_n)_{n\ge 1}$ are not consistent, that is, the measure $\ddot{\mathbf G}_n$ is not the restriction of $\ddot{\mathbf G}_{n+1}$.
	This implies that the change of measure in Theorem \ref{thm: change of measure} is not a martingale change of measure.
\medskip
\begin{proof}[Proof of Theorem \ref{thm: change of measure}]
	Since $(X_m( \mathbf t ))_{0\le m\le n}$ under ${\mathbf G}_n$ is exactly a $\mu$-Galton-Watson process stopped at generation $n$, we have that $\{(X_m( \mathbf t ))_{0\le m\le n}; {\mathbf G}_n\}  \overset{d}{=} (Z_m)_{0\le m\le n}.$
    Similarly, since the process $(X_m( \mathbf t ))_{0\le m\le n}$ under measure $\ddot{\mathbf G}_n$ is constructed as the population of a $k(k-1)$-type size-biased $\mu$-Galton-Watson tree with height $n$, we have that $\{(X_m( \mathbf t ))_{0\le m\le n};\ddot{\mathbf G}_n\}  \overset{d}{=} (\ddot Z_m)_{0\le m\le n}.$
	Equation \eqref{eq:k(k-1)typesizebiasedGWmeasure} can be rewritten in the following form
\begin{equation*}
    	\frac{\ddot{\mathbf G}_n(d \mathbf t )}{\mathbf G_n (d \mathbf t ) }
    =
    	\frac{X_n( \mathbf t )(X_n( \mathbf t )-1)}{n\sigma^2},
    \quad
    	 \mathbf t \in \mathbb T_{\leq n}.
\end{equation*}
    For any bounded Borel function $g$ on $\mathbb N_0^n$, we can then verify that
\begin{equation} \label{eq:proofofchangeofmeasure}
\begin{split}
	&\mathbf E [ g ( \ddot Z_1^{(n)}, \dots, \ddot Z_n^{(n)})]
	= \ddot{\mathbf G}_n [g ( X_1(  \mathbf t ), \dots, X_n(  \mathbf t ))]
    \\ &\quad = {\mathbf G}_n \big[ \frac { X_n( \mathbf t ) ( X_n( \mathbf t ) - 1)} {n \sigma^2} g (X_1( \mathbf t ), \dots, X_n( \mathbf t ))\big]
	\\&\quad = \frac { 1} { n \sigma^2} \mathbf E[ Z_n ( Z_n - 1) g( Z_1, \dots, Z_n)].
\end{split}
\end{equation}
\end{proof}
	Taking $g\equiv 1$ in equation \eqref{eq:proofofchangeofmeasure}, we get that
\begin{equation}
	\label{eq: second moment}
	\mathbf E [Z_n(Z_n-1)]= \mathbf E [\dot Z_n - 1]= n\sigma^2.
\end{equation}
\subsection{Spine decompositions.}
\label{sec:spinesdecomposition}

	Using the notations introduced in the previous subsection, we are now ready to give the precise meaning that \eqref{eq: Our insight} admits a size-biased add-on structure for the size-biased $\mu$-Galton-Watson tree:
\begin{prop}\label{prop: size-biased add-on of size-biased tree }
	Let $(\dot Z_m)_{0 \leq m \leq n}$ be the population of a size-biased $\mu$-Galton watson tree and $(\ddot Z^{(n)}_m)_{0 \leq m \leq n}$ be the population of a $k(k-1)$-type size-biased $\mu$-Galton-Watson tree with height $n$.
	Suppose that $\mu$ satisfies \eqref{eq:mean} and \eqref{eq:variance}.
	Then, we have
\[
	E [ e^{- \lambda \ddot Z_n^{(n)}} ]
	= E [e^{-\lambda \dot Z_n}] E[g(\lambda, \floor{Un})e^{-\lambda \dot Z_{\floor{Un}}}],
\]
where $U$ is a uniform random variable on $[0,1]$ independent of $\{\dot Z_m: 0\le m\le n\}$;
and $g(\lambda, m)$ is a function on $[0,\infty) \times \mathbb N_0$ such that
$g(\lambda, m) \to 1$, uniformly in $\lambda$ as $m\to \infty$.
\end{prop}

\begin{proof}
	
	Let us first recall the spine decomposition for $(\dot T,\dot V)$.
	For any particle $u=u_1\dots u_n$, we define
\begin{equation*}
	[\emptyset, u]
	:= \{u_1\dots u_j:j=0,\dots, n \}
\end{equation*}
	as the \emph{descending family line from $\emptyset$ to $u$}.
	Since $|\dot V|=n$, we must have, for any particle $u\in\mathcal U$, $|[\emptyset, u]\cap\dot V|\in \{0,1,\dots,n\}$.
	Therefore, the particles in $\dot T$ can be separated according to their nearest spine ancestor:
\[
		\dot T
	=
		\bigcup_{k=0}^n\dot A_k
	:=
		\bigcup_{k=0}^n\{u\in\dot T:| [\emptyset, u] \cap \dot V |=k\}.
\]
	This gives a natural decomposition of the $n$th generation population of $\dot T$,
\begin{equation}
\label{eq:generationseperation}
		X_n(\dot T)
	=
		\sum_{k=0}^nX_n(\dot A_k).
\end{equation}
	We note that the right side of the above equation is a summation of independent random variables;
	and from their construction, we see that $X_n(\dot A_k) \overset{d}= Z_{n-k-1}^{(\dot L - 1)}$.
	Here,  $(Z^{(\dot L - 1)}_m)_{m\in \mathbb N_0}$ denotes a $\mu$-Galton-Watson process 
   	with initial population distributed according to $\dot L - 1$.
	For convention, we set $Z^{(\dot L - 1)}_{(-1)}:= 1$.
	Taking Laplace transform on both sides of \eqref{eq:generationseperation} we get
\begin{equation} \label{eq: laplace transform of one-spine decomposition}
	E [e^{-\lambda \dot Z_n}]
	= \prod_{k = 0}^n E[ e^{-\lambda Z^{(\dot L - 1)}_{n-k-1}} ].
\end{equation}
	
	Similarly, consider the $k(k-1)$-type size-biased $\mu$-Galton-Watson tree $(\ddot T,\ddot V,\ddot V')$.
	The particles in $\ddot T$ are first separated into two classes: Say $u \in \ddot T$ is \emph{a descendant from the longer spine} if $[\emptyset , u] \cap (\ddot V' - \ddot V \cap \ddot V') = \emptyset$, and say it is \emph{a descendant from the shorter spine} otherwise.
	Then, the particles in $\ddot T$ is separated according to their nearest spine ancestor:
\begin{equation*}\begin{split}
	\ddot T
	&=\Big( \bigcup_{k=0}^n \{u\in\ddot T: | [\emptyset, u]\cap \ddot V | = k, \text{ $u$ is a descendant from the longer spine}\} \Big)
	\\&\quad \cup \Big( \bigcup_{k= K_n+1}^n \{u\in\ddot T: | [\emptyset, u]\cap \ddot V' | = k, \text{ $u$ is a descendant from the shorter spine}\}  \Big)
	\\&=: 	\Big ( \bigcup_{k=0}^n \ddot A^l_k\Big)  \cup \Big( \bigcup_{k=K_n+1}^n \ddot A^s_k \Big).
\end{split}\end{equation*}
	This gives a natural decomposition of the $n$th generation population of $\ddot T$,
\begin{equation}\label{eq:rawtwospinedecomposition}
		X_n(\ddot T)
	=
		\sum_{k=0}^nX_n(\ddot A^l_k) + \sum_{k=K_n + 1}^n X_n(\ddot A^s_k)
\end{equation}
	We note that, conditioning on $K_n = m$ with $m\in\{0,\dots,n-1\}$, the right side of the above equation is a summation of independent random variables; and from their construction, we see that
	$X_n(\ddot A^l_k) \overset{d}{=} Z^{(\dot L - 1)}_{n-k-1}$ for any $k \neq m$;
	$X_n(\ddot A^l_m) \overset{d}{=} Z^{(\ddot L - 2)}_{n-m-1}$;
	and $X_n(\ddot A^s_k) \overset{d}{=} Z^{(\dot L - 1)}_{n-k-1}$ for each $k \geq m+1$.
	Here, $(Z^{(\dot L - 1)}_k)_{k\ge 0}$ and $(Z^{(\ddot L - 2)}_k)_{k\ge 0}$ are $\mu$-Galton-Watson processes with initial population distributed according to $\dot L-1$ and $\ddot L-2$ respectively.
	For convention, we set $Z^{(\dot L - 1)}_{(-1)}:= 1$ and $Z^{(\ddot L - 2)}_{(-1)}:= 1$.

	Taking Laplace transform on both sides of \eqref{eq:rawtwospinedecomposition}  and using \eqref{eq: laplace transform of one-spine decomposition}, we get
\begin{equation} \label{eq:laplacetransformationoftwospinedecomposition} \begin{split}
	E [ e^{- \lambda \ddot Z_n^{(n)}} ]
	&= \frac{1}{n}\sum_{m=0}^{n-1} \Big( \prod_{k=0,k\neq m}^{n} E[e^{-\lambda Z^{(\dot L - 1)}_{n-k-1}}] \Big) \cdot E [e^{-\lambda Z^{(\ddot L - 2)}_{n-m-1}}] \cdot \Big(\prod_{k= m+1}^n E [e^{-\lambda Z^{(\dot L - 1)}_{n-k-1}}]\Big)
	\\&= E [e^{-\lambda \dot Z_n}]  \frac{1}{n}\sum_{m=0}^{n-1}   \frac{ E [e^{-\lambda Z^{(\ddot L - 2)}_{m}}] }{ E[e^{-\lambda Z^{(\dot L - 1)}_{m}}] } \cdot E[e^{- \lambda \dot Z_{m}}]
	\\&= E [e^{-\lambda \dot Z_n}] E[g(\lambda, \floor{Un})e^{-\lambda \dot Z_{\floor{Un}}}],
\end{split}
\end{equation}
	where
\[
	\mathbf P\{Z^{(\ddot L - 2)}_m=0\}
	\leq	g(\lambda,m)
	: = \frac{ E [e^{-\lambda Z^{(\ddot L - 2)}_{m}}] }{ E[e^{-\lambda Z^{(\dot L - 1)}_{m}}] }
	\leq \mathbf P\{Z^{(\dot L - 1)}_m = 0\}^{-1}.
\]
	Notice that, from the criticality, $\mathbf P\{Z^{(\ddot L - 2)}_m=0\}$ and $\mathbf P\{Z^{(\dot L - 1)}_m = 0\}^{-1}$ converge to $1$.
	The proof is complete.
\end{proof}



\section{Proofs}
\label{sec: proofs}


In order to make comparison between distributions using their size-biased add-on structures, we need the following lemma:
\begin{lem}\label{lem: compare}
	Let $X_0$ and $X_1$ be two non-negative random variables with the same mean $a = E[X_0] = E[X_1] \in (0,\infty)$.
	Let $F_0$ be defined by $E[e^{-\lambda \dot X_0}] = E[e^{-\lambda X_0}] F_0(\lambda)$,  
%	where $\dot X_0$ is the $X_0$-transform of $X_0$,
	where $\dot X_0$ is an $X_0$-transform of $X_0$,
	and $F_1$ be defined by $E[e^{-\lambda \dot X_1}] = E[e^{-\lambda X_1}] F_1(\lambda)$, 
%	where $\dot X_1$ is the $X_1$-transform of $X_1$.
	where $\dot X_1$ is an $X_1$-transform of $X_1$.
	Then,
	\[
  \big| E[e^{-\lambda X_0}] - E[e^{-\lambda X_1}] \big| 
\leq a \int_0^\lambda| F_0(s) - F_1(s) |ds, \quad \lambda \geq 0.
	\]
\end{lem}
\begin{proof}
%	Since $\dot X_0$ is the $X_0$-transform of $X_0$, 
	Since $\dot X_0$ is an $X_0$-transform of $X_0$, 
we have
	\[
	\partial_\lambda ( -\ln E[e^{-\lambda X_0}]) = \frac { E[X_0 e^{-\lambda X_0}]}{ E[e^{-\lambda X_0}] }
	= a F_0(\lambda).
	\]
	Similarly we have $\partial_\lambda ( -\ln E[e^{-\lambda X_1}]) = a F_1(\lambda)$.
	Therefore,
	\[\begin{split}
    \big| E[e^{-\lambda X_0}] - E[e^{-\lambda X_1}] \big|
	&\leq \big| \ln E[e^{-\lambda X_0}] - \ln E[e^{-\lambda X_1}] \big|
	= a\big| \int_0^\lambda F_0(s)ds - \int_0^\lambda F_1(s)ds \big|
	\\& \leq a \int_0^\lambda| F_0(s) - F_1(s) |ds
	\end{split}\]
	as desired.
\end{proof}

%	We are now at the position of proving Lemma \ref{lem: our equation}.
   We are now ready to prove Lemma \ref{lem: our equation}.
	It is elementary to verify that if $Y$ is exponentially distributed, then it satisfies \eqref{eq: x2 type size-biased equation for exponential distribution}.
	So we only have to show that if $Y$ is a strictly positive random variable with finite second moment, then \eqref{eq: x2 type size-biased equation for exponential distribution} implies that it is exponentially distributed.
	The following lemma will be used to prove this.

\begin{lem}\label{lem: zero inequality}
	Suppose that $c>0$ is a constant, and $F$  is a non-negative bounded function on $[0,\infty)$ satisfying that, for any $\lambda\geq 0$,
\begin{equation}\label{eq: zero inequality}
	F(\lambda)
	\leq
	\frac{1}{c}\int_0^1du
	\cdot
	\int_0^\lambda F(us)ds.
\end{equation}
	Then $F\equiv 0$.
\end{lem}
\begin{proof}
	By dividing $\|F\|_\infty$ on the both side of \eqref{eq: zero inequality},
	without loss of any generality, we can assume $F$ is bounded by $1$.
	We first show that $F(\lambda)=0$ for $\lambda \in [0,c)$.
	Using  $F(us)\leq 1$ on the right hand of \eqref{eq: zero inequality}, we get
	\begin{equation*}
		F(\lambda)
		\leq
		\frac{1}{c}\int_0^1du
		\cdot
		\int_0^\lambda ds
		=
		\frac{\lambda}{c},
		\quad
		\lambda\geq 0.
	\end{equation*}
	Plugging this new upper bound into the right hand of \eqref{eq: zero inequality} gives an updated upper bound of $F$:
	\begin{equation*}
		F(\lambda)
		\leq \frac{1}{c}\int_0^1du \cdot \int_0^\lambda \frac{us}{c}ds \leq \frac{1}{c}\int_0^1du \cdot \int_0^\lambda \frac{\lambda}{c}ds
		= \Big(\frac{\lambda}{c}\Big)^2,
		\quad
		\lambda\geq 0.
	\end{equation*}
	Repeating this process, we will have $F(\lambda)\leq (\frac{\lambda}{c})^m$ for any $m>0$. Therefore $F=0$ on $[0,c)$.
	
	To complete the proof, we then show that, for any $k\in\mathbb N$, $F=0$ on $[0,kc)$ implies that $F=0$ on $[0,(k+1)c)$.
	Actually, since $F=0$ on $[0,kc)$, we have
	\begin{equation*}
		F ( k c + \lambda )
		\leq
		\frac { 1 } { c } \int_0^1 du \cdot \int_0^{ k c + \lambda } F ( u s ) ds
		=
		\frac{1}{c}\int_0^1du\cdot\int_{kc}^{kc+\lambda}
		F(us)ds\leq\frac{\lambda}{c}, \quad \lambda\geq 0.
	\end{equation*}
	Iterating, we get that
	\begin{equation*}
		F(kc+\lambda)
		\leq
		\frac{1}{c}\int_0^1du\cdot\int_{kc}^{kc+\lambda} F(us)ds
		\leq
		\frac{1}{c}\int_0^1du\cdot\int_{kc}^{kc+\lambda} \frac{\lambda}{c}ds
		\leq
		\Big(\frac{\lambda}{c}\Big)^2, \quad \lambda\geq 0.
	\end{equation*}
	Repeating this process gives that $F(kc+\lambda)\leq (\frac{\lambda}{c})^m$ for any $m>0$. Therefore $F=0$ on $[0,(k+1)c)$. The rest of the proof follows by induction.
\end{proof}
\begin{proof}[Proof of Lemma \ref{lem: our equation}]
	Suppose that $Y$ is a strictly positive random variable with finite second moment, and \eqref{eq: x2 type size-biased equation for exponential distribution} is true.
	Define
$
	a
	:= E[\dot Y] \in (0,\infty)
$.
% 	Consider a exponential random variable $\mathbf e$ with mean $a/2$.
 	Consider an exponential random variable $\mathbf e$ with mean $a/2$.
	It is elementary to verify that, $\mathbf e$ also satisfies \eqref{eq: x2 type size-biased equation for exponential distribution}, in the sense that
$
	\ddot {\mathbf e} \overset{d} = \dot {\mathbf e}+U\dot {\mathbf e}',
$
	where $\dot {\mathbf e}$ and $\dot {\mathbf e}'$ are 
%	both the $\mathbf e$-transform of $\mathbf e$, 
both $\mathbf e$-transforms of $\mathbf e$, 
%	$\ddot {\mathbf e}$ is the $\mathbf e^2$-transform of $\mathbf e$, 
	$\ddot {\mathbf e}$ is an $\mathbf e^2$-transform of $\mathbf e$, 
	$U$ is a uniform random variable on $[0,1]$, and $\dot {\mathbf e}$, $\dot {\mathbf e}'$, 
%	$\ddot {\mathbf e}$ and $U$ are independent of each other.
	$\ddot {\mathbf e}$ and $U$ are independent.
	Notice that $E[\dot {\mathbf e}] = a$, therefore we can compare the distribution of $\dot Y$ with $\dot {\mathbf e}$ using Lemma \ref{lem: compare}.
	This gives that
\[
	\big|E[ e^{-\lambda \dot Y}] - E[ e^{-\lambda \dot {\mathbf e}}] \big|
	\leq  a  \int_0^\lambda \int_0^1 \big| E [e^{-s u \dot Y}] - E [e^{-s u \dot {\mathbf e}}] \big| du ds,
	\quad \lambda \geq 0,
\]
	which, according to Lemma \ref{lem: zero inequality}, says that 
	$\dot Y \overset{d} = \dot {\mathbf e}$.
	Finally, from \cite[Lemma 2.6]{ArratiaGoldsteinKochman2013} and the fact that $Y$ and $\mathbf e$ are strictly positive, we have
	$Y \overset{d} = \mathbf e$ as desired.
\end{proof}



\begin{proof}[Proof of Theorem \ref{thm: Kolmogrov and Yaglom theorem}\eqref{thm:yaglom}.]
	From Proposition \ref{prop: size-biased add-on of size-biased tree }, we know that
	\[
	E [ e^{- \lambda \ddot Z_n^{(n)}} ]
		= E [e^{-\lambda \dot Z_n}] E[g(\lambda, \floor{Un})e^{-\lambda \dot Z_{\floor{Un}}}],
	\]
	where $U$ is a uniform  random variable on $[0,1]$ independent of $\{\dot Z_m: 0\le m\le n\}$;
	and $g(\lambda, m)$ is a function on $[0,\infty) \times \mathbb N_0$ such that
	$g(\lambda, m) \to 1$, uniformly in $\lambda$ as $m\to \infty$.
	After a renormalization, we have that
\begin{equation}\label{eq: renormalized size-biased add-on equation}
	E [ e^{- \lambda \frac{\ddot Z_n^{(n)}-1}{n}} ]
	= E [e^{-\lambda \frac{\dot Z_n - 1}{n}}] E\big[g\big(\frac{\lambda}{n}, \floor{Un} \big)e^{-\lambda U \frac{\dot Z_{\floor{Un}}}{Un} }\big],
	\quad \lambda \geq 0.
\end{equation}
	According to Theorem \ref{thm: change of measure}, one can verify that 
%	$(\ddot Z_n^{(n)} - 1)/n$ is the $(\dot Z_n - 1)/n$ transform of $(\dot Z_n - 1)/n$.
	$(\ddot Z_n^{(n)} - 1)/n$ is a $(\dot Z_n - 1)/n$ transform of $(\dot Z_n - 1)/n$.
	Therefore, the above equation can be viewed as the size-biased add-on structure for the random variable $(\dot Z_n - 1)/n$.
	Consider an exponential random variable $Y$ with mean $\sigma^2/2$.
%	Let $\dot Y$ be the $Y$-transform of $Y$.
	Let $\dot Y$ be a $Y$-transform of $Y$.
%	Then, it's easy to see that the mean of $\dot Y$ is $\sigma^2$.
Then, it is easy to see that the mean of $\dot Y$ is $\sigma^2$.
	Note that, according to \eqref{eq: second moment}, the mean of $(\dot Z_n - 1)/n$ is also $\sigma^2$.
	Therefore, we can compare the distribution of $(\dot Z_n - 1)/n$ 
%	with $\dot Y$ using Lemma \ref{lem: compare}, which gives that
	with that of $\dot Y$ using Lemma \ref{lem: compare}, which gives that
\[
	\big| E[e^{-\lambda \frac{\dot Z_n - 1}{n}}] - E[e^{-\lambda \dot Y}]\big|
	\leq \sigma^2 \int_0^\lambda ds \int_0^1 \big| g(\frac{s}{n}, \floor{un}) E[e^{-su \frac { Z_{\floor{un}} } {un} }] - E[e^{- su \dot Y}]\big| du.
\]
Taking $n\to \infty$ and using the reverse Fatou's lemma, we arrive at
\[
	M(\lambda)
	\leq \sigma^2 \int_0^1du \int_0^\lambda M(us)ds,
	\quad \lambda\geq 0,
\]
	where
$M(\lambda) := \limsup_{n\to \infty} | E[ e^{- \lambda \frac{\dot Z_n }{n}}] - E[e^{-\lambda \dot Y}]|$.
	Thus by Lemma \ref{lem: zero inequality}, we have $M \equiv 0$, which says that $\dot Z_n/n$ converges weakly to $\dot Y$.

	The rest of the proof follows a standard inverse-size-biased procedure.
	Since $\dot Z_n$ is a $Z_n$-transform of $Z_n$, we have that
	\[\begin{split}
	&E[1-e^{-\lambda \frac{Z_n}{n}} | Z_n > 0]
	= \frac{E[1- e^{-\lambda \frac{Z_n}{n}}]}{P[Z_n > 0]}
	= P[Z_n > 0]^{-1} E \Big[\int_0^\lambda \frac{Z_n}{n}e^{- s \frac{Z_n}{n}} ds\Big]
	\\&\quad = \frac{E[Z_n]}{nP[Z_n > 0]}\int_0^\lambda E [e^{- s \frac{\dot Z_n}{n}}] ds
	\xrightarrow[n\to \infty]{} \frac{\sigma^2}{2} \int_0^\lambda E[e^{-s \dot Y}]ds
	= E[1-e^{-\lambda Y}].
	\end{split}\]
	The proof is complete.
\end{proof}
%%%------Bibliography-------------------
\vspace{.1in}
\begin{thebibliography}{99}
\bibitem{ArratiaGoldsteinKochman2013}
	Arratia R., Goldstein L. and Kochman F.:
	Size bias for one and all,
   {\it arXiv:1308.2729}
	(2013).
\bibitem{AN}
	Athreya, K.  and  Ney, P.:
	Functionals of critical multitype branching processes,
	{\it Ann. Probab.}
	{\bf 2} (1974), 339--343.
\bibitem{geiger1999elementary}
	Geiger, J.:
	Elementary new proofs of classical limit theorems for Galton--Watson processes.
	{\it J. Appl. Probab.}
	\textbf{36} (1999), 310--309.
\bibitem{geiger2000new}
	Geiger, J.:
	A new proof of Yaglom's exponential limit law.
	In {\it Mathematics and Computer Science},
	pp. 245--249. Springer, 2000.
\bibitem{harris2015many}
	Harris, S. and Roberts, M.:
	The many-to-few lemma and multiple spines.
	{\it Ann.  Inst. Henri Poincar{\'e}, Probab. Stat.}
	\textbf{53} (2017), 226--242.
\bibitem{kesten1966galton}
	Kesten, H.,  Ney, P and Spitzer, F.:
	The Galton-Watson process with mean one and finite variance.
	{\it Theory Probab. Appl.}
	\textbf{11} (1966), 513--540.
\bibitem{kolmogorov1938losung}
	Kolmogorov, A. N.:
	Zur l{\"o}sung einer biologischen aufgabe.
	{\it Comm. Math. Mech. Chebyshev Univ. Tomsk}
	\textbf{2} (1938), 1--12.
\bibitem{lyons1995conceptual}
	Lyons, R.,  Pemantle, R. and Peres, Y.:
	Conceptual Proofs of $ L \log L $ criteria for mean behavior of branching processes.
	{\it Ann. Probab.} \textbf{23} (1995), 1125--1138.
\bibitem{RenSongSun2017Spine}
	Ren, Y.-X., Song, R. and Sun, Z.:
	Spine decompositions and limit theorems for a class of critical superprocesses
	{\it arXiv preprint arXiv:1711.09188}
	(2017).
\bibitem{VD}
	Vatutin, V. A. and Dyakonova,  E. E.:
	The survival probability of a critical mutitype Galton-Watson branching process.
	{\it J.  Math. Sci.}
	\textbf{106} (2001), 2752--2759.
\bibitem{yaglom1947certain}
	Yaglom, A. M.:
	Certain limit theorems of the theory of branching random processes.
	{\it Doklady Akad. Nauk SSSR (NS)}
	\textbf{56} (1947), 795--798.
\bibitem{Zubkov1975}
	Zubkov, A. M.:
	Limiting distributions of the distance to the closest common ancestor.
	{\it Theory Prob. Appl.}
	\textbf{20} (1975), 602-612.

\end{thebibliography}
\end{document}
